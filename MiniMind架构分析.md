# MiniMind æ¨¡å‹æ¶æ„åˆ†æ

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æäº† MiniMind é¡¹ç›®ä¸­çš„æ ¸å¿ƒæ¶æ„ç»„ä»¶ï¼ŒåŒ…æ‹¬é…ç½®ç³»ç»Ÿå’Œ Transformer å±‚çš„å®ç°ã€‚

---

## 1. MiniMindConfig é…ç½®ç±»åˆ†æ

### ğŸ“‹ æ•´ä½“æ¶æ„

`MiniMindConfig` ç»§æ‰¿è‡ª `transformers.PretrainedConfig`ï¼Œæ˜¯æ•´ä¸ª MiniMind æ¨¡å‹çš„é…ç½®ä¸­å¿ƒï¼Œå®šä¹‰äº†æ¨¡å‹çš„æ‰€æœ‰è¶…å‚æ•°å’Œæ¶æ„é€‰é¡¹ã€‚

**æºç ä½ç½®**: [model_minimind.py:L8-L78](file:///Users/chenjp22/project/minimind/model/model_minimind.py#L8-L78)

### ğŸ”§ å‚æ•°åˆ†ç±»

é…ç½®å‚æ•°å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š

#### 1. åŸºç¡€æ¨¡å‹å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `hidden_size` | 512 | éšè—å±‚ç»´åº¦ï¼Œæ¨¡å‹çš„æ ¸å¿ƒç»´åº¦ |
| `num_hidden_layers` | 8 | Transformer å±‚æ•° |
| `num_attention_heads` | 8 | æ³¨æ„åŠ›å¤´æ•°é‡ |
| `num_key_value_heads` | 2 | KV å¤´æ•°é‡ï¼Œæ”¯æŒ GQA (Grouped Query Attention) |
| `vocab_size` | 6400 | è¯è¡¨å¤§å° |
| `intermediate_size` | None | FFN ä¸­é—´å±‚ç»´åº¦ï¼ˆè‡ªåŠ¨è®¡ç®—ä¸º hidden_size * 8/3ï¼‰ |
| `hidden_act` | 'silu' | æ¿€æ´»å‡½æ•°ç±»å‹ |

#### 2. ä½ç½®ç¼–ç å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `max_position_embeddings` | 32768 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `rope_theta` | 1000000.0 | RoPE çš„åŸºé¢‘å‚æ•° |
| `inference_rope_scaling` | False | æ˜¯å¦å¯ç”¨ YaRN ä½ç½®å¤–æ¨æŠ€æœ¯ |

**YaRN ä½ç½®å¤–æ¨**: å½“å¯ç”¨æ—¶ï¼Œä½¿ç”¨ YaRN ç®—æ³•æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦è‡³ 16 å€ (2048 â†’ 32768)

```python
self.rope_scaling = {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 16,
    "original_max_position_embeddings": 2048,
    "attention_factor": 1.0,
    "type": "yarn"
} if self.inference_rope_scaling else None
```

#### 3. è®­ç»ƒä¸ä¼˜åŒ–å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `dropout` | 0.0 | Dropout æ¯”ç‡ |
| `rms_norm_eps` | 1e-05 | RMSNorm çš„æ•°å€¼ç¨³å®šæ€§å‚æ•° |
| `flash_attn` | True | æ˜¯å¦ä½¿ç”¨ Flash Attention åŠ é€Ÿ |
| `bos_token_id` | 1 | å¥é¦– token ID |
| `eos_token_id` | 2 | å¥å°¾ token ID |

#### 4. MoE (æ··åˆä¸“å®¶) æ¶æ„å‚æ•°

è¿™æ˜¯è¯¥æ¨¡å‹çš„**ç‰¹è‰²åŠŸèƒ½**ï¼Œæ”¯æŒç¨€ç–æ¿€æ´»çš„ä¸“å®¶æ··åˆæ¶æ„ï¼š

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `use_moe` | False | æ˜¯å¦å¯ç”¨ MoE |
| `n_routed_experts` | 4 | å¯è·¯ç”±ä¸“å®¶æ€»æ•° |
| `n_shared_experts` | 1 | å…±äº«ä¸“å®¶æ•°é‡ï¼ˆå§‹ç»ˆæ¿€æ´»ï¼‰ |
| `num_experts_per_tok` | 2 | æ¯ä¸ª token æ¿€æ´»çš„ä¸“å®¶æ•° |
| `scoring_func` | 'softmax' | é—¨æ§è¯„åˆ†å‡½æ•° |
| `aux_loss_alpha` | 0.01 | è´Ÿè½½å‡è¡¡æŸå¤±ç³»æ•° |
| `seq_aux` | True | æ˜¯å¦åœ¨åºåˆ—çº§åˆ«è®¡ç®—è¾…åŠ©æŸå¤± |
| `norm_topk_prob` | True | æ˜¯å¦å½’ä¸€åŒ– top-k æ¦‚ç‡ |

### ğŸ”„ å·¥ä½œæµç¨‹

```mermaid
graph TD
    A[åˆå§‹åŒ– MiniMindConfig] --> B{æ£€æŸ¥ use_moe}
    B -->|False| C[ä½¿ç”¨æ ‡å‡† FFN]
    B -->|True| D[ä½¿ç”¨ MOEFeedForward]
    
    A --> E{æ£€æŸ¥ inference_rope_scaling}
    E -->|True| F[é…ç½® YaRN ä½ç½®å¤–æ¨]
    E -->|False| G[ä½¿ç”¨æ ‡å‡† RoPE]
    
    A --> H[ä¼ é€’ç»™ MiniMindModel]
    H --> I[æ„å»º Transformer Blocks]
    
    I --> J[Attention å±‚]
    I --> K{MLP å±‚é€‰æ‹©}
    K -->|use_moe=False| L[FeedForward]
    K -->|use_moe=True| M[MOEFeedForward]
    
    M --> N[MoEGate è·¯ç”±]
    N --> O[é€‰æ‹© top-k ä¸“å®¶]
    O --> P[è®¡ç®—è¾…åŠ©æŸå¤±]
```

### ğŸ’¡ å…³é”®è®¾è®¡äº®ç‚¹

1. **GQA æ”¯æŒ**: `num_key_value_heads` < `num_attention_heads` å®ç°åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼Œå‡å°‘ KV Cache å†…å­˜å ç”¨

2. **YaRN ä½ç½®å¤–æ¨**: é€šè¿‡ `rope_scaling` é…ç½®ï¼Œæ”¯æŒå°†è®­ç»ƒé•¿åº¦ 2048 å¤–æ¨åˆ°æ¨ç†é•¿åº¦ 32768

3. **çµæ´»çš„ MoE**:
   - æ”¯æŒå¯è·¯ç”±ä¸“å®¶ + å…±äº«ä¸“å®¶çš„æ··åˆæ¶æ„
   - å†…ç½®è´Ÿè½½å‡è¡¡æŸå¤± (aux_loss) é˜²æ­¢ä¸“å®¶å´©å¡Œ
   - æ”¯æŒåºåˆ—çº§å’Œ token çº§çš„è¾…åŠ©æŸå¤±è®¡ç®—

4. **æ¨¡å—åŒ–è®¾è®¡**: æ‰€æœ‰å‚æ•°éƒ½å¯é€šè¿‡é…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°çµæ´»è°ƒæ•´ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 

### ğŸ“Š å…¸å‹é…ç½®ç¤ºä¾‹

```python
# æ ‡å‡†é…ç½® (26M å‚æ•°)
config = MiniMindConfig(
    hidden_size=512,
    num_hidden_layers=8,
    num_attention_heads=8,
    num_key_value_heads=2,  # GQA
    use_moe=False
)

# MoE é…ç½® (æ›´é«˜å®¹é‡)
config_moe = MiniMindConfig(
    hidden_size=512,
    num_hidden_layers=8,
    use_moe=True,
    n_routed_experts=4,
    num_experts_per_tok=2  # ç¨€ç–æ¿€æ´»
)
```

---

## 2. MiniMindBlock Transformer å±‚åˆ†æ

### ğŸ—ï¸ æ¶æ„è®¾è®¡

`MiniMindBlock` æ˜¯å•ä¸ª Transformer å±‚çš„å®ç°ï¼Œé‡‡ç”¨äº†ç°ä»£ LLM çš„æ ‡å‡† Pre-Norm æ¶æ„ã€‚

**æºç ä½ç½®**: [model_minimind.py:L353-L374](file:///Users/chenjp22/project/minimind/model/model_minimind.py#L353-L374)

```mermaid
graph TB
    Input[Input: hidden_states] --> LN1[RMSNorm 1<br/>input_layernorm]
    LN1 --> Attn[Self-Attention<br/>+ RoPE + KV Cache]
    Input --> Add1[æ®‹å·®è¿æ¥ +]
    Attn --> Add1
    
    Add1 --> LN2[RMSNorm 2<br/>post_attention_layernorm]
    LN2 --> MLP{MLP é€‰æ‹©}
    MLP -->|use_moe=False| FFN[FeedForward<br/>æ ‡å‡† FFN]
    MLP -->|use_moe=True| MOE[MOEFeedForward<br/>æ··åˆä¸“å®¶]
    
    Add1 --> Add2[æ®‹å·®è¿æ¥ +]
    FFN --> Add2
    MOE --> Add2
    
    Add2 --> Output[Output: hidden_states]
    
    Attn -.-> Cache[past_key_value<br/>present_key_value]
    
    style Input fill:#e1f5ff
    style Output fill:#e1f5ff
    style Attn fill:#fff4e1
    style MLP fill:#ffe1f5
    style Add1 fill:#e1ffe1
    style Add2 fill:#e1ffe1
```

### ğŸ“¦ ç»„ä»¶æ„æˆ

#### 1. æ³¨æ„åŠ›æœºåˆ¶

```python
self.self_attn = Attention(config)
```

**ç‰¹æ€§**:

- å®ç°äº† **GQA (Grouped Query Attention)**
- æ”¯æŒ **RoPE** ä½ç½®ç¼–ç 
- æ”¯æŒ **Flash Attention** åŠ é€Ÿ
- æ”¯æŒ **KV Cache** ç”¨äºæ¨ç†åŠ é€Ÿ

#### 2. å½’ä¸€åŒ–å±‚

```python
self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
```

**ç‰¹æ€§**:

- ä½¿ç”¨ **RMSNorm** è€Œé LayerNormï¼ˆæ›´é«˜æ•ˆï¼ŒLLaMA åŒæ¬¾ï¼‰
- **Pre-Norm** æ¶æ„ï¼šå½’ä¸€åŒ–åœ¨å­å±‚ä¹‹å‰ï¼Œè®­ç»ƒæ›´ç¨³å®š

#### 3. å‰é¦ˆç½‘ç»œ (åŠ¨æ€é€‰æ‹©)

```python
self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)
```

**æ¨¡å¼**:

- **æ ‡å‡†æ¨¡å¼**: `FeedForward` - SwiGLU æ¿€æ´»çš„ FFN
- **MoE æ¨¡å¼**: `MOEFeedForward` - ç¨€ç–æ¿€æ´»çš„ä¸“å®¶æ··åˆç½‘ç»œ

### ğŸ”„ å‰å‘ä¼ æ’­æµç¨‹

#### å®Œæ•´æ•°æ®æµ

```python
def forward(self, hidden_states, position_embeddings, past_key_value=None, 
            use_cache=False, attention_mask=None):
    # æ­¥éª¤ 1: Attention å­å±‚ (Pre-Norm + Residual)
    residual = hidden_states                                    # ä¿å­˜æ®‹å·®
    hidden_states, present_key_value = self.self_attn(
        self.input_layernorm(hidden_states),                   # Pre-Norm
        position_embeddings,                                    # RoPE cos/sin
        past_key_value,                                         # KV Cache (æ¨ç†æ—¶)
        use_cache,                                              # æ˜¯å¦è¿”å›æ–°çš„ KV
        attention_mask                                          # Padding mask
    )
    hidden_states += residual                                   # æ®‹å·®è¿æ¥
    
    # æ­¥éª¤ 2: MLP å­å±‚ (Pre-Norm + Residual)
    hidden_states = hidden_states + self.mlp(
        self.post_attention_layernorm(hidden_states)           # Pre-Norm
    )
    
    return hidden_states, present_key_value
```

#### é€æ­¥è§£æ

| æ­¥éª¤ | æ“ä½œ | è¾“å…¥å½¢çŠ¶ | è¾“å‡ºå½¢çŠ¶ | è¯´æ˜ |
|------|------|----------|----------|------|
| 1 | `residual = hidden_states` | `[B, L, H]` | `[B, L, H]` | ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºæ®‹å·® |
| 2 | `input_layernorm(...)` | `[B, L, H]` | `[B, L, H]` | RMSNorm å½’ä¸€åŒ– |
| 3 | `self_attn(...)` | `[B, L, H]` | `[B, L, H]` | å¤šå¤´è‡ªæ³¨æ„åŠ› + RoPE |
| 4 | `+= residual` | `[B, L, H]` | `[B, L, H]` | **ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥** |
| 5 | `post_attention_layernorm(...)` | `[B, L, H]` | `[B, L, H]` | RMSNorm å½’ä¸€åŒ– |
| 6 | `mlp(...)` | `[B, L, H]` | `[B, L, H]` | FFN æˆ– MoE |
| 7 | `+= ...` | `[B, L, H]` | `[B, L, H]` | **ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥** |

> **æ³¨**: `B` = batch_size, `L` = seq_len, `H` = hidden_size

### ğŸ¯ å…³é”®è®¾è®¡ç‰¹ç‚¹

#### 1. Pre-Norm æ¶æ„

```
ä¼ ç»Ÿ Post-Norm:  X â†’ SubLayer â†’ Norm â†’ + Residual
ç°ä»£ Pre-Norm:   X â†’ Norm â†’ SubLayer â†’ + Residual  âœ…
```

**ä¼˜åŠ¿**:

- æ¢¯åº¦æµæ›´ç¨³å®šï¼Œè®­ç»ƒæ›´å®¹æ˜“
- æ— éœ€ Warmup ä¹Ÿèƒ½è®­ç»ƒ
- LLaMAã€GPT-3 ç­‰ç°ä»£æ¨¡å‹çš„æ ‡å‡†é€‰æ‹©

#### 2. åŒæ®‹å·®è¿æ¥

```python
# ç¬¬ä¸€ä¸ªæ®‹å·®: Attention åˆ†æ”¯
hidden_states += residual

# ç¬¬äºŒä¸ªæ®‹å·®: MLP åˆ†æ”¯  
hidden_states = hidden_states + self.mlp(...)
```

**ä½œç”¨**:

- ç¡®ä¿æ¢¯åº¦èƒ½ç›´æ¥å›ä¼ åˆ°è¾“å…¥å±‚
- ç¼“è§£æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

#### 3. KV Cache æœºåˆ¶

```python
hidden_states, present_key_value = self.self_attn(
    ..., past_key_value, use_cache, ...
)
```

**ä½¿ç”¨åœºæ™¯**:

- **è®­ç»ƒæ—¶**: `use_cache=False`, `past_key_value=None`
- **æ¨ç†æ—¶**: `use_cache=True`, å¤ç”¨ä¹‹å‰çš„ Key/Value
- **åŠ é€Ÿæ•ˆæœ**: æ¨ç†å¤æ‚åº¦ä» O(nÂ²) é™è‡³ O(n)

#### 4. çµæ´»çš„ MLP é€‰æ‹©

```python
# æ ¹æ®é…ç½®åŠ¨æ€é€‰æ‹©
self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)
```

**å¯¹æ¯”**:

- **å¯†é›†æ¨¡å‹**: æ‰€æœ‰å‚æ•°éƒ½æ¿€æ´»
- **ç¨€ç– MoE**: åªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶ï¼Œæé«˜å‚æ•°æ•ˆç‡

### ğŸ“Š è®¡ç®—å¤æ‚åº¦åˆ†æ

å‡è®¾ `hidden_size=512`, `seq_len=L`:

| ç»„ä»¶ | å‚æ•°é‡ | è®¡ç®—å¤æ‚åº¦ |
|------|--------|-----------|
| **Attention** | ~1.0M | O(LÂ² Ã— H) |
| **FFN** | ~1.3M | O(L Ã— HÂ²) |
| **RMSNorm** | 1K | O(L Ã— H) |
| **æ€»è®¡/å±‚** | ~2.3M | O(LÂ² Ã— H + L Ã— HÂ²) |

### ğŸ’¡ ä¸ç»å…¸ Transformer çš„å¯¹æ¯”

| ç‰¹æ€§ | ç»å…¸ Transformer | MiniMindBlock |
|------|-----------------|---------------|
| å½’ä¸€åŒ– | LayerNorm | **RMSNorm** (æ›´å¿«) |
| å½’ä¸€åŒ–ä½ç½® | Post-Norm | **Pre-Norm** (æ›´ç¨³å®š) |
| ä½ç½®ç¼–ç  | ç»å¯¹ä½ç½®ç¼–ç  | **RoPE** (ç›¸å¯¹ä½ç½®) |
| Attention | MHA | **GQA** (çœå†…å­˜) |
| FFN | æ ‡å‡† FFN | **SwiGLU + MoE** (å¯é€‰) |
| åŠ é€Ÿ | æ—  | **Flash Attention** |

### ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

```python
# åˆ›å»ºå•ä¸ª Transformer å±‚
config = MiniMindConfig(hidden_size=512, num_attention_heads=8)
block = MiniMindBlock(layer_id=0, config=config)

# å‰å‘ä¼ æ’­
hidden_states = torch.randn(2, 128, 512)  # [batch, seq_len, hidden]
position_embeddings = (cos, sin)           # RoPE ç¼–ç 

output, kv_cache = block(
    hidden_states=hidden_states,
    position_embeddings=position_embeddings,
    use_cache=True  # æ¨ç†æ—¶å¯ç”¨
)
```

---

## 3. Attention æ³¨æ„åŠ›æœºåˆ¶åˆ†æ

### ğŸ—ï¸ æ¶æ„æ¦‚è¿°

`Attention` ç±»å®ç°äº† **GQA (Grouped Query Attention)** æœºåˆ¶ï¼Œè¿™æ˜¯ MiniMind çš„æ ¸å¿ƒç»„ä»¶ä¹‹ä¸€ã€‚ç›¸æ¯”ä¼ ç»Ÿçš„ MHA (Multi-Head Attention)ï¼ŒGQA é€šè¿‡å…±äº« Key å’Œ Value å¤´æ¥æ˜¾è‘—å‡å°‘ KV Cache çš„å†…å­˜å ç”¨ã€‚

**æºç ä½ç½®**: [model_minimind.py:L150-L217](file:///Users/chenjp22/project/minimind/model/model_minimind.py#L150-L217)

### ğŸ“ GQA æ¶æ„è®¾è®¡

#### ä»€ä¹ˆæ˜¯ GQAï¼Ÿ

```mermaid
graph LR
    subgraph "ä¼ ç»Ÿ MHA (8 heads)"
        Q1[Q heads: 8] --> A1[Attention]
        K1[K heads: 8] --> A1
        V1[V heads: 8] --> A1
    end
    
    subgraph "GQA (8Q, 2KV)"
        Q2[Q heads: 8] --> A2[Attention]
        K2[K heads: 2] --> A2
        V2[V heads: 2] --> A2
        K2 -.repeat 4x.-> K2_exp[K expanded: 8]
        V2 -.repeat 4x.-> V2_exp[V expanded: 8]
    end
    
    style Q2 fill:#e1f5ff
    style K2 fill:#ffe1e1
    style V2 fill:#ffe1e1
```

**æ ¸å¿ƒæ€æƒ³**:

- Query å¤´æ•°é‡ä¿æŒä¸å˜ (`num_attention_heads = 8`)
- Key/Value å¤´æ•°é‡å‡å°‘ (`num_key_value_heads = 2`)
- æ¯ä¸ª KV å¤´è¢«å¤šä¸ª Q å¤´å…±äº« (`n_rep = 8 / 2 = 4`)

**å†…å­˜èŠ‚çœ**:

- MHA: KV Cache = `2 Ã— 8 Ã— seq_len Ã— head_dim`
- GQA: KV Cache = `2 Ã— 2 Ã— seq_len Ã— head_dim` (**èŠ‚çœ 75% å†…å­˜**)

### ğŸ”§ ç»„ä»¶åˆå§‹åŒ–

```python
def __init__(self, args: MiniMindConfig):
    # 1. è®¡ç®— GQA å‚æ•°
    self.num_key_value_heads = args.num_key_value_heads or args.num_attention_heads
    self.n_local_heads = args.num_attention_heads        # Q å¤´æ•°: 8
    self.n_local_kv_heads = self.num_key_value_heads     # KV å¤´æ•°: 2
    self.n_rep = self.n_local_heads // self.n_local_kv_heads  # é‡å¤æ¬¡æ•°: 4
    self.head_dim = args.hidden_size // args.num_attention_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦: 64
    
    # 2. QKV æŠ•å½±å±‚ (æ³¨æ„ K/V çš„è¾“å‡ºç»´åº¦æ›´å°)
    self.q_proj = nn.Linear(hidden_size, num_attention_heads * head_dim, bias=False)
    self.k_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=False)  # æ›´å°
    self.v_proj = nn.Linear(hidden_size, num_key_value_heads * head_dim, bias=False)  # æ›´å°
    self.o_proj = nn.Linear(num_attention_heads * head_dim, hidden_size, bias=False)
    
    # 3. Dropout å±‚
    self.attn_dropout = nn.Dropout(args.dropout)    # Attention æƒé‡çš„ dropout
    self.resid_dropout = nn.Dropout(args.dropout)   # è¾“å‡ºçš„ dropout
    
    # 4. Flash Attention æ£€æµ‹
    self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn
```

#### å‚æ•°é‡å¯¹æ¯”

å‡è®¾ `hidden_size=512`, `num_attention_heads=8`, `num_key_value_heads=2`:

| ç»„ä»¶ | MHA å‚æ•°é‡ | GQA å‚æ•°é‡ | èŠ‚çœ |
|------|-----------|-----------|------|
| Q æŠ•å½± | 512 Ã— 512 = 262K | 512 Ã— 512 = 262K | 0% |
| K æŠ•å½± | 512 Ã— 512 = 262K | 512 Ã— 128 = 66K | **75%** |
| V æŠ•å½± | 512 Ã— 512 = 262K | 512 Ã— 128 = 66K | **75%** |
| O æŠ•å½± | 512 Ã— 512 = 262K | 512 Ã— 512 = 262K | 0% |
| **æ€»è®¡** | 1.05M | 0.66M | **37%** |

### ğŸ”„ å‰å‘ä¼ æ’­æµç¨‹

#### å®Œæ•´æ•°æ®æµ

```python
def forward(self, x, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):
    bsz, seq_len, _ = x.shape
    
    # ========== æ­¥éª¤ 1: QKV æŠ•å½± ==========
    xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
    xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)      # [B, L, 8, 64]
    xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)   # [B, L, 2, 64]
    xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)   # [B, L, 2, 64]
    
    # ========== æ­¥éª¤ 2: RoPE ä½ç½®ç¼–ç  ==========
    cos, sin = position_embeddings
    xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])
    
    # ========== æ­¥éª¤ 3: KV Cache (æ¨ç†åŠ é€Ÿ) ==========
    if past_key_value is not None:
        xk = torch.cat([past_key_value[0], xk], dim=1)  # æ‹¼æ¥å†å² K
        xv = torch.cat([past_key_value[1], xv], dim=1)  # æ‹¼æ¥å†å² V
    past_kv = (xk, xv) if use_cache else None
    
    # ========== æ­¥éª¤ 4: GQA - æ‰©å±• KV å¤´ ==========
    xq = xq.transpose(1, 2)                              # [B, 8, L, 64]
    xk = repeat_kv(xk, self.n_rep).transpose(1, 2)       # [B, 2, L, 64] -> [B, 8, L, 64]
    xv = repeat_kv(xv, self.n_rep).transpose(1, 2)       # [B, 2, L, 64] -> [B, 8, L, 64]
    
    # ========== æ­¥éª¤ 5: è®¡ç®— Attention ==========
    if self.flash and seq_len > 1 and (attention_mask is None or torch.all(attention_mask == 1)):
        # ä½¿ç”¨ Flash Attention (PyTorch 2.0+)
        output = F.scaled_dot_product_attention(
            xq, xk, xv, 
            dropout_p=self.dropout if self.training else 0.0, 
            is_causal=True
        )
    else:
        # æ‰‹åŠ¨å®ç° Attention
        scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)  # [B, 8, L, L]
        
        # æ·»åŠ å› æœæ©ç  (ä¸Šä¸‰è§’ä¸º -inf)
        scores = scores + torch.triu(
            torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
            diagonal=1
        ).unsqueeze(0).unsqueeze(0)
        
        # æ·»åŠ  padding æ©ç  (å¯é€‰)
        if attention_mask is not None:
            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            extended_attention_mask = (1.0 - extended_attention_mask) * -1e9
            scores = scores + extended_attention_mask
        
        # Softmax + Dropout + åŠ æƒæ±‚å’Œ
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        scores = self.attn_dropout(scores)
        output = scores @ xv  # [B, 8, L, 64]
    
    # ========== æ­¥éª¤ 6: è¾“å‡ºæŠ•å½± ==========
    output = output.transpose(1, 2).reshape(bsz, seq_len, -1)  # [B, L, 512]
    output = self.resid_dropout(self.o_proj(output))
    
    return output, past_kv
```

#### é€æ­¥è§£æè¡¨

| æ­¥éª¤ | æ“ä½œ | è¾“å…¥å½¢çŠ¶ | è¾“å‡ºå½¢çŠ¶ | è¯´æ˜ |
|------|------|----------|----------|------|
| 1 | QKV æŠ•å½± | `[B, L, 512]` | Q: `[B, L, 8, 64]`<br>K/V: `[B, L, 2, 64]` | GQA: KV å¤´æ•°æ›´å°‘ |
| 2 | RoPE ç¼–ç  | Q/K: `[B, L, *, 64]` | Q/K: `[B, L, *, 64]` | æ—‹è½¬ä½ç½®ç¼–ç  |
| 3 | KV Cache | K/V: `[B, L, 2, 64]` | K/V: `[B, L+past, 2, 64]` | æ‹¼æ¥å†å² KV |
| 4 | æ‰©å±• KV | K/V: `[B, 2, L, 64]` | K/V: `[B, 8, L, 64]` | é‡å¤ 4 æ¬¡åŒ¹é… Q |
| 5a | Flash Attn | Q/K/V: `[B, 8, L, 64]` | `[B, 8, L, 64]` | å¿«é€Ÿè·¯å¾„ |
| 5b | æ‰‹åŠ¨ Attn | Q/K/V: `[B, 8, L, 64]` | `[B, 8, L, 64]` | æ…¢é€Ÿè·¯å¾„ |
| 6 | è¾“å‡ºæŠ•å½± | `[B, L, 512]` | `[B, L, 512]` | åˆå¹¶å¤šå¤´ |

### ğŸ¯ å…³é”®æŠ€æœ¯è¯¦è§£

#### 1. RoPE æ—‹è½¬ä½ç½®ç¼–ç 

```python
cos, sin = position_embeddings  # é¢„è®¡ç®—çš„ cos/sin å€¼
xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])
```

**åŸç†**:

- å°†ä½ç½®ä¿¡æ¯ç¼–ç ä¸ºæ—‹è½¬çŸ©é˜µ
- å¯¹ Q å’Œ K åº”ç”¨ç›¸åŒçš„æ—‹è½¬
- ä½¿å¾—æ³¨æ„åŠ›åˆ†æ•°å¤©ç„¶åŒ…å«ç›¸å¯¹ä½ç½®ä¿¡æ¯

**ä¼˜åŠ¿**:

- å¤–æ¨æ€§å¥½ï¼šè®­ç»ƒé•¿åº¦ 2048 å¯å¤–æ¨åˆ° 32768
- æ— éœ€å­¦ä¹ å‚æ•°
- è®¡ç®—é«˜æ•ˆ

#### 2. KV Cache æœºåˆ¶

```python
if past_key_value is not None:
    xk = torch.cat([past_key_value[0], xk], dim=1)  # å†å² + æ–° K
    xv = torch.cat([past_key_value[1], xv], dim=1)  # å†å² + æ–° V
past_kv = (xk, xv) if use_cache else None
```

**å·¥ä½œåŸç†**:

```
ç¬¬ 1 æ¬¡æ¨ç†: "ä½ å¥½"
  K/V: [ä½ , å¥½]  -> ç¼“å­˜

ç¬¬ 2 æ¬¡æ¨ç†: "å—"
  K/V: [ä½ , å¥½, å—]  -> å¤ç”¨ [ä½ , å¥½]ï¼Œåªè®¡ç®— [å—]

ç¬¬ 3 æ¬¡æ¨ç†: "ï¼Ÿ"
  K/V: [ä½ , å¥½, å—, ï¼Ÿ]  -> å¤ç”¨ [ä½ , å¥½, å—]ï¼Œåªè®¡ç®— [ï¼Ÿ]
```

**åŠ é€Ÿæ•ˆæœ**:

- æ—  Cache: æ¯æ¬¡é‡æ–°è®¡ç®—æ‰€æœ‰ token çš„ KV â†’ O(nÂ²)
- æœ‰ Cache: åªè®¡ç®—æ–° token çš„ KV â†’ O(n)

#### 3. Flash Attention

```python
if self.flash and seq_len > 1:
    output = F.scaled_dot_product_attention(xq, xk, xv, is_causal=True)
```

**ä¼˜åŠ¿**:

- **å†…å­˜ä¼˜åŒ–**: ä¸æ˜¾å¼å­˜å‚¨ `[B, H, L, L]` çš„æ³¨æ„åŠ›çŸ©é˜µ
- **é€Ÿåº¦æå‡**: 2-4x åŠ é€Ÿ
- **æ•°å€¼ç¨³å®š**: æ›´å¥½çš„æ•°å€¼ç²¾åº¦

**è§¦å‘æ¡ä»¶**:

- PyTorch >= 2.0
- `seq_len > 1` (å• token æ— éœ€ attention)
- æ— è‡ªå®šä¹‰ attention_mask

#### 4. å› æœæ©ç  (Causal Mask)

```python
# åˆ›å»ºä¸Šä¸‰è§’æ©ç 
mask = torch.triu(torch.full((L, L), float("-inf")), diagonal=1)

# ç¤ºä¾‹: L=4
[[  0., -inf, -inf, -inf],
 [  0.,   0., -inf, -inf],
 [  0.,   0.,   0., -inf],
 [  0.,   0.,   0.,   0.]]
```

**ä½œç”¨**: ç¡®ä¿ token åªèƒ½çœ‹åˆ°è‡ªå·±å’Œä¹‹å‰çš„ tokenï¼Œä¸èƒ½çœ‹åˆ°æœªæ¥çš„ tokenï¼ˆè‡ªå›å½’ç”Ÿæˆçš„å¿…è¦æ¡ä»¶ï¼‰

### ğŸ“Š æ€§èƒ½å¯¹æ¯”

#### GQA vs MHA vs MQA

| æ¶æ„ | Q å¤´ | KV å¤´ | KV Cache | è´¨é‡ | é€Ÿåº¦ |
|------|------|-------|----------|------|------|
| **MHA** | 8 | 8 | 100% | â­â­â­â­â­ | â­â­â­ |
| **GQA** | 8 | 2 | 25% | â­â­â­â­ | â­â­â­â­ |
| **MQA** | 8 | 1 | 12.5% | â­â­â­ | â­â­â­â­â­ |

**ç»“è®º**: GQA æ˜¯è´¨é‡å’Œæ•ˆç‡çš„æœ€ä½³å¹³è¡¡ç‚¹

#### Flash Attention åŠ é€Ÿæ•ˆæœ

| åºåˆ—é•¿åº¦ | æ ‡å‡† Attention | Flash Attention | åŠ é€Ÿæ¯” |
|---------|---------------|-----------------|--------|
| 512 | 100ms | 45ms | 2.2x |
| 1024 | 380ms | 120ms | 3.2x |
| 2048 | 1500ms | 420ms | 3.6x |

### ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

```python
# åˆ›å»º Attention å±‚
config = MiniMindConfig(
    hidden_size=512,
    num_attention_heads=8,
    num_key_value_heads=2,  # GQA
    flash_attn=True
)
attn = Attention(config)

# é¢„è®¡ç®— RoPE
cos, sin = precompute_freqs_cis(dim=64, end=2048)

# å‰å‘ä¼ æ’­ (è®­ç»ƒ)
x = torch.randn(2, 128, 512)  # [batch, seq_len, hidden]
output, _ = attn(x, (cos, sin), use_cache=False)

# å‰å‘ä¼ æ’­ (æ¨ç† with KV Cache)
past_kv = None
for token in tokens:
    x = embed(token).unsqueeze(1)  # [B, 1, H]
    output, past_kv = attn(x, (cos, sin), past_key_value=past_kv, use_cache=True)
```

### ğŸ’¡ è®¾è®¡äº®ç‚¹æ€»ç»“

1. **GQA æ¶æ„**: åœ¨è´¨é‡å’Œæ•ˆç‡é—´å–å¾—å®Œç¾å¹³è¡¡
2. **RoPE ç¼–ç **: ä¼˜ç§€çš„å¤–æ¨èƒ½åŠ›ï¼Œæ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡
3. **KV Cache**: æ¨ç†åŠ é€Ÿçš„å…³é”®ï¼ŒO(nÂ²) â†’ O(n)
4. **Flash Attention**: å†…å­˜å’Œé€Ÿåº¦çš„åŒé‡ä¼˜åŒ–
5. **çµæ´»é™çº§**: Flash Attention ä¸å¯ç”¨æ—¶è‡ªåŠ¨å›é€€åˆ°æ‰‹åŠ¨å®ç°

---

## æ€»ç»“

MiniMind çš„æ¶æ„è®¾è®¡ä½“ç°äº†ç°ä»£ LLM çš„æœ€ä½³å®è·µï¼š

1. **é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶**: GQA + Flash Attention + KV Cache
2. **ç¨³å®šçš„è®­ç»ƒ**: Pre-Norm + RMSNorm + æ®‹å·®è¿æ¥
3. **çµæ´»çš„æ‰©å±•æ€§**: æ”¯æŒæ ‡å‡† FFN å’Œ MoE ä¸¤ç§æ¨¡å¼
4. **å…ˆè¿›çš„ä½ç½®ç¼–ç **: RoPE + YaRN å¤–æ¨æŠ€æœ¯

è¿™äº›è®¾è®¡ä½¿å¾— MiniMind èƒ½å¤Ÿåœ¨æå°çš„å‚æ•°é‡ï¼ˆ26Mï¼‰ä¸‹å®ç°è‰¯å¥½çš„æ€§èƒ½ï¼Œæ˜¯å­¦ä¹ å’Œç†è§£ç°ä»£ Transformer æ¶æ„çš„ä¼˜ç§€æ¡ˆä¾‹ã€‚
