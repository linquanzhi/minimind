# MiniMind æ¨¡å‹æ¶æ„åˆ†æ

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æäº† MiniMind é¡¹ç›®ä¸­çš„æ ¸å¿ƒæ¶æ„ç»„ä»¶ï¼ŒåŒ…æ‹¬é…ç½®ç³»ç»Ÿå’Œ Transformer å±‚çš„å®ç°ã€‚

---

## 1. MiniMindConfig é…ç½®ç±»åˆ†æ

### ğŸ“‹ æ•´ä½“æ¶æ„

`MiniMindConfig` ç»§æ‰¿è‡ª `transformers.PretrainedConfig`ï¼Œæ˜¯æ•´ä¸ª MiniMind æ¨¡å‹çš„é…ç½®ä¸­å¿ƒï¼Œå®šä¹‰äº†æ¨¡å‹çš„æ‰€æœ‰è¶…å‚æ•°å’Œæ¶æ„é€‰é¡¹ã€‚

**æºç ä½ç½®**: [model_minimind.py:L8-L78](file:///Users/chenjp22/project/minimind/model/model_minimind.py#L8-L78)

### ğŸ”§ å‚æ•°åˆ†ç±»

é…ç½®å‚æ•°å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š

#### 1. åŸºç¡€æ¨¡å‹å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `hidden_size` | 512 | éšè—å±‚ç»´åº¦ï¼Œæ¨¡å‹çš„æ ¸å¿ƒç»´åº¦ |
| `num_hidden_layers` | 8 | Transformer å±‚æ•° |
| `num_attention_heads` | 8 | æ³¨æ„åŠ›å¤´æ•°é‡ |
| `num_key_value_heads` | 2 | KV å¤´æ•°é‡ï¼Œæ”¯æŒ GQA (Grouped Query Attention) |
| `vocab_size` | 6400 | è¯è¡¨å¤§å° |
| `intermediate_size` | None | FFN ä¸­é—´å±‚ç»´åº¦ï¼ˆè‡ªåŠ¨è®¡ç®—ä¸º hidden_size * 8/3ï¼‰ |
| `hidden_act` | 'silu' | æ¿€æ´»å‡½æ•°ç±»å‹ |

#### 2. ä½ç½®ç¼–ç å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `max_position_embeddings` | 32768 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `rope_theta` | 1000000.0 | RoPE çš„åŸºé¢‘å‚æ•° |
| `inference_rope_scaling` | False | æ˜¯å¦å¯ç”¨ YaRN ä½ç½®å¤–æ¨æŠ€æœ¯ |

**YaRN ä½ç½®å¤–æ¨**: å½“å¯ç”¨æ—¶ï¼Œä½¿ç”¨ YaRN ç®—æ³•æ‰©å±•ä¸Šä¸‹æ–‡é•¿åº¦è‡³ 16 å€ (2048 â†’ 32768)

```python
self.rope_scaling = {
    "beta_fast": 32,
    "beta_slow": 1,
    "factor": 16,
    "original_max_position_embeddings": 2048,
    "attention_factor": 1.0,
    "type": "yarn"
} if self.inference_rope_scaling else None
```

#### 3. è®­ç»ƒä¸ä¼˜åŒ–å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `dropout` | 0.0 | Dropout æ¯”ç‡ |
| `rms_norm_eps` | 1e-05 | RMSNorm çš„æ•°å€¼ç¨³å®šæ€§å‚æ•° |
| `flash_attn` | True | æ˜¯å¦ä½¿ç”¨ Flash Attention åŠ é€Ÿ |
| `bos_token_id` | 1 | å¥é¦– token ID |
| `eos_token_id` | 2 | å¥å°¾ token ID |

#### 4. MoE (æ··åˆä¸“å®¶) æ¶æ„å‚æ•°

è¿™æ˜¯è¯¥æ¨¡å‹çš„**ç‰¹è‰²åŠŸèƒ½**ï¼Œæ”¯æŒç¨€ç–æ¿€æ´»çš„ä¸“å®¶æ··åˆæ¶æ„ï¼š

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `use_moe` | False | æ˜¯å¦å¯ç”¨ MoE |
| `n_routed_experts` | 4 | å¯è·¯ç”±ä¸“å®¶æ€»æ•° |
| `n_shared_experts` | 1 | å…±äº«ä¸“å®¶æ•°é‡ï¼ˆå§‹ç»ˆæ¿€æ´»ï¼‰ |
| `num_experts_per_tok` | 2 | æ¯ä¸ª token æ¿€æ´»çš„ä¸“å®¶æ•° |
| `scoring_func` | 'softmax' | é—¨æ§è¯„åˆ†å‡½æ•° |
| `aux_loss_alpha` | 0.01 | è´Ÿè½½å‡è¡¡æŸå¤±ç³»æ•° |
| `seq_aux` | True | æ˜¯å¦åœ¨åºåˆ—çº§åˆ«è®¡ç®—è¾…åŠ©æŸå¤± |
| `norm_topk_prob` | True | æ˜¯å¦å½’ä¸€åŒ– top-k æ¦‚ç‡ |

### ğŸ”„ å·¥ä½œæµç¨‹

```mermaid
graph TD
    A[åˆå§‹åŒ– MiniMindConfig] --> B{æ£€æŸ¥ use_moe}
    B -->|False| C[ä½¿ç”¨æ ‡å‡† FFN]
    B -->|True| D[ä½¿ç”¨ MOEFeedForward]
    
    A --> E{æ£€æŸ¥ inference_rope_scaling}
    E -->|True| F[é…ç½® YaRN ä½ç½®å¤–æ¨]
    E -->|False| G[ä½¿ç”¨æ ‡å‡† RoPE]
    
    A --> H[ä¼ é€’ç»™ MiniMindModel]
    H --> I[æ„å»º Transformer Blocks]
    
    I --> J[Attention å±‚]
    I --> K{MLP å±‚é€‰æ‹©}
    K -->|use_moe=False| L[FeedForward]
    K -->|use_moe=True| M[MOEFeedForward]
    
    M --> N[MoEGate è·¯ç”±]
    N --> O[é€‰æ‹© top-k ä¸“å®¶]
    O --> P[è®¡ç®—è¾…åŠ©æŸå¤±]
```

### ğŸ’¡ å…³é”®è®¾è®¡äº®ç‚¹

1. **GQA æ”¯æŒ**: `num_key_value_heads` < `num_attention_heads` å®ç°åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼Œå‡å°‘ KV Cache å†…å­˜å ç”¨

2. **YaRN ä½ç½®å¤–æ¨**: é€šè¿‡ `rope_scaling` é…ç½®ï¼Œæ”¯æŒå°†è®­ç»ƒé•¿åº¦ 2048 å¤–æ¨åˆ°æ¨ç†é•¿åº¦ 32768

3. **çµæ´»çš„ MoE**:
   - æ”¯æŒå¯è·¯ç”±ä¸“å®¶ + å…±äº«ä¸“å®¶çš„æ··åˆæ¶æ„
   - å†…ç½®è´Ÿè½½å‡è¡¡æŸå¤± (aux_loss) é˜²æ­¢ä¸“å®¶å´©å¡Œ
   - æ”¯æŒåºåˆ—çº§å’Œ token çº§çš„è¾…åŠ©æŸå¤±è®¡ç®—

4. **æ¨¡å—åŒ–è®¾è®¡**: æ‰€æœ‰å‚æ•°éƒ½å¯é€šè¿‡é…ç½®æ–‡ä»¶æˆ–å‘½ä»¤è¡Œå‚æ•°çµæ´»è°ƒæ•´ï¼Œæ— éœ€ä¿®æ”¹ä»£ç 

### ğŸ“Š å…¸å‹é…ç½®ç¤ºä¾‹

```python
# æ ‡å‡†é…ç½® (26M å‚æ•°)
config = MiniMindConfig(
    hidden_size=512,
    num_hidden_layers=8,
    num_attention_heads=8,
    num_key_value_heads=2,  # GQA
    use_moe=False
)

# MoE é…ç½® (æ›´é«˜å®¹é‡)
config_moe = MiniMindConfig(
    hidden_size=512,
    num_hidden_layers=8,
    use_moe=True,
    n_routed_experts=4,
    num_experts_per_tok=2  # ç¨€ç–æ¿€æ´»
)
```

---

## 2. MiniMindBlock Transformer å±‚åˆ†æ

### ğŸ—ï¸ æ¶æ„è®¾è®¡

`MiniMindBlock` æ˜¯å•ä¸ª Transformer å±‚çš„å®ç°ï¼Œé‡‡ç”¨äº†ç°ä»£ LLM çš„æ ‡å‡† Pre-Norm æ¶æ„ã€‚

**æºç ä½ç½®**: [model_minimind.py:L353-L374](file:///Users/chenjp22/project/minimind/model/model_minimind.py#L353-L374)

```mermaid
graph TB
    Input[Input: hidden_states] --> LN1[RMSNorm 1<br/>input_layernorm]
    LN1 --> Attn[Self-Attention<br/>+ RoPE + KV Cache]
    Input --> Add1[æ®‹å·®è¿æ¥ +]
    Attn --> Add1
    
    Add1 --> LN2[RMSNorm 2<br/>post_attention_layernorm]
    LN2 --> MLP{MLP é€‰æ‹©}
    MLP -->|use_moe=False| FFN[FeedForward<br/>æ ‡å‡† FFN]
    MLP -->|use_moe=True| MOE[MOEFeedForward<br/>æ··åˆä¸“å®¶]
    
    Add1 --> Add2[æ®‹å·®è¿æ¥ +]
    FFN --> Add2
    MOE --> Add2
    
    Add2 --> Output[Output: hidden_states]
    
    Attn -.-> Cache[past_key_value<br/>present_key_value]
    
    style Input fill:#e1f5ff
    style Output fill:#e1f5ff
    style Attn fill:#fff4e1
    style MLP fill:#ffe1f5
    style Add1 fill:#e1ffe1
    style Add2 fill:#e1ffe1
```

### ğŸ“¦ ç»„ä»¶æ„æˆ

#### 1. æ³¨æ„åŠ›æœºåˆ¶

```python
self.self_attn = Attention(config)
```

**ç‰¹æ€§**:

- å®ç°äº† **GQA (Grouped Query Attention)**
- æ”¯æŒ **RoPE** ä½ç½®ç¼–ç 
- æ”¯æŒ **Flash Attention** åŠ é€Ÿ
- æ”¯æŒ **KV Cache** ç”¨äºæ¨ç†åŠ é€Ÿ

#### 2. å½’ä¸€åŒ–å±‚

```python
self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
```

**ç‰¹æ€§**:

- ä½¿ç”¨ **RMSNorm** è€Œé LayerNormï¼ˆæ›´é«˜æ•ˆï¼ŒLLaMA åŒæ¬¾ï¼‰
- **Pre-Norm** æ¶æ„ï¼šå½’ä¸€åŒ–åœ¨å­å±‚ä¹‹å‰ï¼Œè®­ç»ƒæ›´ç¨³å®š

#### 3. å‰é¦ˆç½‘ç»œ (åŠ¨æ€é€‰æ‹©)

```python
self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)
```

**æ¨¡å¼**:

- **æ ‡å‡†æ¨¡å¼**: `FeedForward` - SwiGLU æ¿€æ´»çš„ FFN
- **MoE æ¨¡å¼**: `MOEFeedForward` - ç¨€ç–æ¿€æ´»çš„ä¸“å®¶æ··åˆç½‘ç»œ

### ğŸ”„ å‰å‘ä¼ æ’­æµç¨‹

#### å®Œæ•´æ•°æ®æµ

```python
def forward(self, hidden_states, position_embeddings, past_key_value=None, 
            use_cache=False, attention_mask=None):
    # æ­¥éª¤ 1: Attention å­å±‚ (Pre-Norm + Residual)
    residual = hidden_states                                    # ä¿å­˜æ®‹å·®
    hidden_states, present_key_value = self.self_attn(
        self.input_layernorm(hidden_states),                   # Pre-Norm
        position_embeddings,                                    # RoPE cos/sin
        past_key_value,                                         # KV Cache (æ¨ç†æ—¶)
        use_cache,                                              # æ˜¯å¦è¿”å›æ–°çš„ KV
        attention_mask                                          # Padding mask
    )
    hidden_states += residual                                   # æ®‹å·®è¿æ¥
    
    # æ­¥éª¤ 2: MLP å­å±‚ (Pre-Norm + Residual)
    hidden_states = hidden_states + self.mlp(
        self.post_attention_layernorm(hidden_states)           # Pre-Norm
    )
    
    return hidden_states, present_key_value
```

#### é€æ­¥è§£æ

| æ­¥éª¤ | æ“ä½œ | è¾“å…¥å½¢çŠ¶ | è¾“å‡ºå½¢çŠ¶ | è¯´æ˜ |
|------|------|----------|----------|------|
| 1 | `residual = hidden_states` | `[B, L, H]` | `[B, L, H]` | ä¿å­˜åŸå§‹è¾“å…¥ç”¨äºæ®‹å·® |
| 2 | `input_layernorm(...)` | `[B, L, H]` | `[B, L, H]` | RMSNorm å½’ä¸€åŒ– |
| 3 | `self_attn(...)` | `[B, L, H]` | `[B, L, H]` | å¤šå¤´è‡ªæ³¨æ„åŠ› + RoPE |
| 4 | `+= residual` | `[B, L, H]` | `[B, L, H]` | **ç¬¬ä¸€ä¸ªæ®‹å·®è¿æ¥** |
| 5 | `post_attention_layernorm(...)` | `[B, L, H]` | `[B, L, H]` | RMSNorm å½’ä¸€åŒ– |
| 6 | `mlp(...)` | `[B, L, H]` | `[B, L, H]` | FFN æˆ– MoE |
| 7 | `+= ...` | `[B, L, H]` | `[B, L, H]` | **ç¬¬äºŒä¸ªæ®‹å·®è¿æ¥** |

> **æ³¨**: `B` = batch_size, `L` = seq_len, `H` = hidden_size

### ğŸ¯ å…³é”®è®¾è®¡ç‰¹ç‚¹

#### 1. Pre-Norm æ¶æ„

```
ä¼ ç»Ÿ Post-Norm:  X â†’ SubLayer â†’ Norm â†’ + Residual
ç°ä»£ Pre-Norm:   X â†’ Norm â†’ SubLayer â†’ + Residual  âœ…
```

**ä¼˜åŠ¿**:

- æ¢¯åº¦æµæ›´ç¨³å®šï¼Œè®­ç»ƒæ›´å®¹æ˜“
- æ— éœ€ Warmup ä¹Ÿèƒ½è®­ç»ƒ
- LLaMAã€GPT-3 ç­‰ç°ä»£æ¨¡å‹çš„æ ‡å‡†é€‰æ‹©

#### 2. åŒæ®‹å·®è¿æ¥

```python
# ç¬¬ä¸€ä¸ªæ®‹å·®: Attention åˆ†æ”¯
hidden_states += residual

# ç¬¬äºŒä¸ªæ®‹å·®: MLP åˆ†æ”¯  
hidden_states = hidden_states + self.mlp(...)
```

**ä½œç”¨**:

- ç¡®ä¿æ¢¯åº¦èƒ½ç›´æ¥å›ä¼ åˆ°è¾“å…¥å±‚
- ç¼“è§£æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

#### 3. KV Cache æœºåˆ¶

```python
hidden_states, present_key_value = self.self_attn(
    ..., past_key_value, use_cache, ...
)
```

**ä½¿ç”¨åœºæ™¯**:

- **è®­ç»ƒæ—¶**: `use_cache=False`, `past_key_value=None`
- **æ¨ç†æ—¶**: `use_cache=True`, å¤ç”¨ä¹‹å‰çš„ Key/Value
- **åŠ é€Ÿæ•ˆæœ**: æ¨ç†å¤æ‚åº¦ä» O(nÂ²) é™è‡³ O(n)

#### 4. çµæ´»çš„ MLP é€‰æ‹©

```python
# æ ¹æ®é…ç½®åŠ¨æ€é€‰æ‹©
self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)
```

**å¯¹æ¯”**:

- **å¯†é›†æ¨¡å‹**: æ‰€æœ‰å‚æ•°éƒ½æ¿€æ´»
- **ç¨€ç– MoE**: åªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶ï¼Œæé«˜å‚æ•°æ•ˆç‡

### ğŸ“Š è®¡ç®—å¤æ‚åº¦åˆ†æ

å‡è®¾ `hidden_size=512`, `seq_len=L`:

| ç»„ä»¶ | å‚æ•°é‡ | è®¡ç®—å¤æ‚åº¦ |
|------|--------|-----------|
| **Attention** | ~1.0M | O(LÂ² Ã— H) |
| **FFN** | ~1.3M | O(L Ã— HÂ²) |
| **RMSNorm** | 1K | O(L Ã— H) |
| **æ€»è®¡/å±‚** | ~2.3M | O(LÂ² Ã— H + L Ã— HÂ²) |

### ğŸ’¡ ä¸ç»å…¸ Transformer çš„å¯¹æ¯”

| ç‰¹æ€§ | ç»å…¸ Transformer | MiniMindBlock |
|------|-----------------|---------------|
| å½’ä¸€åŒ– | LayerNorm | **RMSNorm** (æ›´å¿«) |
| å½’ä¸€åŒ–ä½ç½® | Post-Norm | **Pre-Norm** (æ›´ç¨³å®š) |
| ä½ç½®ç¼–ç  | ç»å¯¹ä½ç½®ç¼–ç  | **RoPE** (ç›¸å¯¹ä½ç½®) |
| Attention | MHA | **GQA** (çœå†…å­˜) |
| FFN | æ ‡å‡† FFN | **SwiGLU + MoE** (å¯é€‰) |
| åŠ é€Ÿ | æ—  | **Flash Attention** |

### ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

```python
# åˆ›å»ºå•ä¸ª Transformer å±‚
config = MiniMindConfig(hidden_size=512, num_attention_heads=8)
block = MiniMindBlock(layer_id=0, config=config)

# å‰å‘ä¼ æ’­
hidden_states = torch.randn(2, 128, 512)  # [batch, seq_len, hidden]
position_embeddings = (cos, sin)           # RoPE ç¼–ç 

output, kv_cache = block(
    hidden_states=hidden_states,
    position_embeddings=position_embeddings,
    use_cache=True  # æ¨ç†æ—¶å¯ç”¨
)
```

---

## æ€»ç»“

MiniMind çš„æ¶æ„è®¾è®¡ä½“ç°äº†ç°ä»£ LLM çš„æœ€ä½³å®è·µï¼š

1. **é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶**: GQA + Flash Attention + KV Cache
2. **ç¨³å®šçš„è®­ç»ƒ**: Pre-Norm + RMSNorm + æ®‹å·®è¿æ¥
3. **çµæ´»çš„æ‰©å±•æ€§**: æ”¯æŒæ ‡å‡† FFN å’Œ MoE ä¸¤ç§æ¨¡å¼
4. **å…ˆè¿›çš„ä½ç½®ç¼–ç **: RoPE + YaRN å¤–æ¨æŠ€æœ¯

è¿™äº›è®¾è®¡ä½¿å¾— MiniMind èƒ½å¤Ÿåœ¨æå°çš„å‚æ•°é‡ï¼ˆ26Mï¼‰ä¸‹å®ç°è‰¯å¥½çš„æ€§èƒ½ï¼Œæ˜¯å­¦ä¹ å’Œç†è§£ç°ä»£ Transformer æ¶æ„çš„ä¼˜ç§€æ¡ˆä¾‹ã€‚
